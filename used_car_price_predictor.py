# -*- coding: utf-8 -*-
"""Used_Car_Price_Predictor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t7IMo7Lzn6UOrx74R8K4ta-D9l_fdbEW
"""

import pandas as pd
import seaborn as sns
import numpy as np

pd.set_option('display.max_columns', None)

import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('used_cars_dataDB.csv')

data.head()

data.info()

data.isnull().mean()

threshold = len(data) * 0.5
data = data.dropna(thresh=threshold, axis=1)
data.isnull().mean()

data.head(3)

cols = ['back_legroom','body_type','city','city_fuel_economy','daysonmarket','dealer_zip','engine_cylinders','engine_displacement','engine_type','frame_damaged','fuel_type','has_accidents','height','highway_fuel_economy','horsepower','isCab','is_new','make_name','maximum_seating','mileage','owner_count','price','transmission','year']
data = data[cols]

data.head()

data.info()

data['back_legroom'] = pd.to_numeric(data['back_legroom'].str.replace(' in', ''), errors='coerce')
data['height'] = pd.to_numeric(data['height'].str.replace(' in', ''), errors='coerce')
data['maximum_seating'] = pd.to_numeric(data['maximum_seating'].str.replace(' seats', ''), errors='coerce')

data.head(2)

data['dealer_zip'] = data['dealer_zip'].astype(str)

data['frame_damaged'].value_counts()

data['has_accidents'].value_counts()

data['isCab'].value_counts()

bool_map = {True: 1, False: 0, '1': 1, '0': 0}
data['frame_damaged'] = data['frame_damaged'].map(bool_map)
data['has_accidents'] = data['has_accidents'].map(bool_map)
data['isCab'] = data['isCab'].map(bool_map)

data['isCab'].value_counts()

data['engine_cylinders'].value_counts()

# Get the counts of each category
counts = data['engine_cylinders'].value_counts()
# Find the categories to be replaced
to_replace = counts[counts < 100].index
# Replace these categories with 'Other'
data['engine_cylinders'] = data['engine_cylinders'].replace(to_replace, 'Other')
# Get the counts of each category
data['engine_cylinders'].value_counts()

# Loop over all columns
for column in data.columns:
    # Check if the column is of object type
    if data[column].dtype == 'object':
        # Get a count of unique values and print it
        print(f'Column Name: {column}')
        print(f'Number of unique Categories: {data[column].nunique()}')
        print(data[column].value_counts())
        print('-' * 50)  # Print a separator for readability

# Loop over all columns
for column in data.columns:
    # Check if the column is of object type
    if data[column].dtype == 'object':
        # Get a count of unique values
        counts = data[column].value_counts(normalize=True)

        # Find the categories to be replaced
        to_replace = counts[counts < 0.01].index

        # Replace these categories with 'Other'
        data[column] = data[column].replace(to_replace, 'Other')

# Loop over all columns
for column in data.columns:
    # Check if the column is of object type
    if data[column].dtype == 'object':
        # Get a count of unique values and print it
        print(f'Column Name: {column}')
        print(f'Number of unique Categories: {data[column].nunique()}')
        print(data[column].value_counts())
        print('-' * 50)  # Print a separator for readability

#Define X and y
X = data.drop(['price'],axis = 1)
y = data['price']

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 42)

X_train.shape , X_test.shape , y_train.shape , y_test.shape

# Define preprocessing for numeric columns (scale them)
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

# Compute median of each numeric feature on the training set
medians = X_train[numeric_features].median()

# Use these medians to fill NaNs in both the training and test set
X_train[numeric_features] = X_train[numeric_features].fillna(medians)
X_test[numeric_features] = X_test[numeric_features].fillna(medians)

# Fill NaNs with 'Others' in both training and test set
X_train[categorical_features] = X_train[categorical_features].fillna('Others')
X_test[categorical_features] = X_test[categorical_features].fillna('Others')

X_train.isnull().sum().sum() , X_test.isnull().sum().sum()

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
X_test[numeric_features] = scaler.transform(X_test[numeric_features])

# Transform categorical variables
X_train = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)
X_test = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)

# Align the train and test sets to ensure they have the same dummy variable columns
X_train, X_test = X_train.align(X_test, join='left', axis=1)

# Fill NaN values created by align function with 0
X_test = X_test.fillna(0)

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import time

def evaluate_regression(true_values, predicted_values):
    # Calculate metrics
    mse = mean_squared_error(true_values, predicted_values)
    mae = mean_absolute_error(true_values, predicted_values)
    r2 = r2_score(true_values, predicted_values)
    
    # Print metrics
    print(f"Mean Absolute Error: {mae}")
    print(f"R-squared Score: {r2}")

# Train and evaluate Linear Regression Model
start_time = time.time()
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)
lr_train_time = time.time() - start_time
# Print evaluation results for Linear Regression
print("Linear Regression:")
evaluate_regression(y_test, lr_predictions)
print(f"Training Time: {lr_train_time} seconds")
print('-' * 50)

# Train and evaluate Decision Tree Regression Model
start_time = time.time()
dt_model = DecisionTreeRegressor()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)
dt_train_time = time.time() - start_time

# Print evaluation results for Decision Tree
print("Decision Tree Regression:")
evaluate_regression(y_test, dt_predictions)
print(f"Training Time: {dt_train_time} seconds")
print('-' * 50)

# Train and evaluate Random Forest Regression Model
start_time = time.time()
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_train_time = time.time() - start_time

# Print evaluation results for Random Forest
print("Random Forest Regression:")
evaluate_regression(y_test, rf_predictions)
print(f"Training Time: {rf_train_time} seconds")
print('-' * 50)

# Train and evaluate Gradient Boosted Tree Regression Model
start_time = time.time()
gbt_model = GradientBoostingRegressor()
gbt_model.fit(X_train, y_train)
gbt_predictions = gbt_model.predict(X_test)
gbt_train_time = time.time() - start_time

# Print evaluation results for Gradient Boosted Tree
print("Gradient Boosted Tree Regression:")
evaluate_regression(y_test, gbt_predictions)
print(f"Training Time: {gbt_train_time} seconds")
print('-' * 50)

from sklearn.feature_selection import SelectFromModel
import matplotlib.pyplot as plt

# Get feature importances and sort them in descending order
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(15, 15))
plt.title("Feature Importances")
plt.barh(range(X_train.shape[1]), importances[indices], align="center")
plt.yticks(range(X_train.shape[1]), X_train.columns[indices])
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# Select top k features based on feature importances
k = 10
top_features = X_train.columns[indices[:k]]
print(top_features)

# Subset the training and testing data with the top features
X_train_selected = X_train[top_features]
X_test_selected = X_test[top_features]

# Retrain and evaluate the models with selected features
models = [
    LinearRegression(),
    DecisionTreeRegressor(),
    RandomForestRegressor(),
    GradientBoostingRegressor()
]

for model in models:
    model_name = model.__class__.__name__
    start_time = time.time()
    model.fit(X_train_selected, y_train)
    predictions = model.predict(X_test_selected)
    train_time = time.time() - start_time

    print(f"{model_name} with Top {k} Features:")
    evaluate_regression(y_test, predictions)
    print(f"Training Time: {train_time} seconds")
    print('-' * 50)

